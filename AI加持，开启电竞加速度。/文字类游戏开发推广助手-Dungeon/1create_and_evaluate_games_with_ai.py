# -*- coding: utf-8 -*-
"""create and evaluate games with AI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1shstAQ-5Jxz2feY705BZco0BAGofi1J5

# Demo 1 evaluate game with AI
Text Game is one type of popular game that you play it by typing text in a game built with text.

TextWorld is a microsoft open source text game.

In this demo, I would use TextWorld to setup a Text game, and Use Rainforce learning to play the game and evaluate the game.

## Environment setup

* TextWorld (microsoft upstream version is being actively developed, I froze a version)
"""

!apt install build-essential libffi-dev python3-dev curl git
!pip install git+https://github.com/zhangabner/TextWorld > /dev/null 2>&1
!pip install ptan > /dev/null 2>&1
!pip3 install torch torchvision > /dev/null 2>&1

# Juggling around colab requirement
!pip uninstall -y prompt-toolkit > /dev/null 2>&1
!pip install prompt-toolkit==1.0.16
# DO NOT FORGET to restart runtime as suggested

"""## restart the runtime

## Textworld basic test

## Build games we'll play with

TextWorld allows to generate games of specific complexity, which makes it great for experimentation, learning and reasearch, as we can gradually increase the difficulty of the game by increasing complexity of problems.

Below we generate the problem with 10 objects, 5 rooms and quest length 5
"""

import gym
import textworld.gym as tw_gym
import os
from google.colab import drive
drive.mount('/content/drive')

cd drive/

ls

os.mkdir("/content/drive/MyDrive/mlheart")

os.mkdir("/content/drive/MyDrive/mlheart/games")
os.mkdir("/content/drive/MyDrive/mlheart/traininglogs")
os.mkdir("/content/drive/MyDrive/mlheart/playlogs")







import ptan
import random
import pathlib
import logging
import numpy as np
from typing import Tuple, List, Iterable, Optional, Any
from textworld.gym import spaces as tw_spaces
from textworld.envs.wrappers.filter import EnvInfos

"""## Env encoder wrapper

To simplify further experiments, below is the wrapper around TextWorld environment which transforms the observations using `Word.tokenize()` method

"""

STEPS_LIMIT = 50

EXTRA_GAME_INFO = {
  "inventory": True,
  "description": True,
  "intermediate_reward": True,
  "admissible_commands": True,
}

import time
import datetime
import itertools
import torch.optim as optim
import matplotlib.pylab as plt

ENC_SIZE = 20
EMB_SIZE = 20
REPLAY_SIZE = 10000
REPLAY_INITIAL = 100
GAMMA = 0.9
LEARNING_RATE = 5e-5
SYNC_NETS = 100
BATCH_SIZE = 64

INITIAL_EPSILON = 1.0
FINAL_EPSILON = 0.2
STEPS_EPSILON = 1000



class TextWorldPreproc(gym.Wrapper):
    """
    Simple wrapper to preprocess text_world game observation
    """
    log = logging.getLogger("TextWorldPreproc")
    
    def __init__(self, env: gym.Env, encode_raw_text: bool = False,
                 encode_extra_fields: Iterable[str] = ('description', 'inventory'),
                 use_admissible_commands: bool = True,
                 use_intermediate_reward: bool = True,
                 tokens_limit: Optional[int] = None):
        """
        :param env: env to be wrapped. Has to provide Word observations
        :param encode_raw_text: do we need to encode raw observation from environment, if true, adds an extra encoder
        :param encode_extra_fields: tuple of field names to be encoded, expected to be string values
        :param use_admissible_commands: if true, admissible commands used for action wrapping
        :param use_intermediate_reward: take intermediate reward into account
        :param tokens_limit: optional limit of tokens in the encoded fields
        """
        super(TextWorldPreproc, self).__init__(env)
        if not isinstance(env.observation_space, tw_spaces.Word):
            raise ValueError("Env should expose text_world compatible observation space, "
                             "this one gives %s" % env.observation_space)
        self._encode_raw_text = encode_raw_text
        self._encode_extra_field = tuple(encode_extra_fields)
        self._use_admissible_commands = use_admissible_commands
        self._use_intermedate_reward = use_intermediate_reward
        self._num_fields = len(self._encode_extra_field) + int(self._encode_raw_text)
        self._last_admissible_commands = None
        self._last_extra_info = None
        self._tokens_limit = tokens_limit
        self._cmd_hist = []

    @property
    def num_fields(self):
        return self._num_fields

    def _encode(self, obs: str, extra_info: dict) -> dict:
        obs_result = []
        if self._encode_raw_text:
            tokens = self.env.observation_space.tokenize(obs)
            if self._tokens_limit is not None:
                tokens = tokens[:self._tokens_limit]
            obs_result.append(tokens)
        for field in self._encode_extra_field:
            tokens = self.env.observation_space.tokenize(extra_info[field])
            if self._tokens_limit is not None:
                tokens = tokens[:self._tokens_limit]
            obs_result.append(tokens)
        result = {"obs": obs_result}
        if self._use_admissible_commands:
            adm_result = []
            for cmd in extra_info['admissible_commands']:
                adm_result.append(self.env.action_space.tokenize(cmd))
            result['admissible_commands'] = adm_result
            self._last_admissible_commands = extra_info['admissible_commands']
        self._last_extra_info = extra_info
        return result

    # TextWorld environment has a workaround of gym drawback: 
    # reset returns tuple with raw observation and extra dict
    def reset(self):
        res = self.env.reset()
        self._cmd_hist = []
        return self._encode(res[0], res[1])

    def step(self, action):
        if self._use_admissible_commands:
            action = self._last_admissible_commands[action]
            self._cmd_hist.append(action)
        obs, r, is_done, extra = self.env.step(action)
        if self._use_intermedate_reward:
            r += extra.get('intermediate_reward', 0)
        new_extra = dict(extra)
        for f in self._encode_extra_field + ('admissible_commands', 'intermediate_reward'):
            if f in new_extra:
                new_extra.pop(f)
        # if is_done:
        #     self.log.info("Commands: %s", self._cmd_hist)
        #     self.log.info("Reward: %s, extra: %s", r, new_extra)
        return self._encode(obs, extra), r, is_done, new_extra

    @property
    def last_admissible_commands(self):
        return tuple(self._last_admissible_commands) if self._last_admissible_commands else None

    @property
    def last_extra_info(self):
        return self._last_extra_info

import logging
import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils
import torch.nn.functional as F
import numpy as np
class Encoder(nn.Module):
    """
    Takes input sequences (after embeddings) and returns the hidden state from LSTM
    """
    def __init__(self, emb_size: int, out_size: int):
        super(Encoder, self).__init__()

        self.net = nn.LSTM(input_size=emb_size, hidden_size=out_size, batch_first=True)

    def forward(self, x):
        self.net.flatten_parameters()
        _, hid_cell = self.net(x)
        # Warn: if bidir=True or several layers, sequeeze has to be changed!
        return hid_cell[0].squeeze(0)
class Preprocessor(nn.Module):
    """
    Takes batch of several input sequences and outputs their summary from one or many encoders
    """
    def __init__(self, dict_size: int, emb_size: int, num_sequences: int, enc_output_size: int):
        """
        :param dict_size: amount of words is our vocabulary
        :param emb_size: dimensionality of embeddings
        :param num_sequences: count of sequences
        :param enc_output_size: output from single encoder
        """
        super(Preprocessor, self).__init__()

        self.emb = nn.Embedding(num_embeddings=dict_size, embedding_dim=emb_size)
        self.encoders = []
        for idx in range(num_sequences):
            enc = Encoder(emb_size, enc_output_size)
            self.encoders.append(enc)
            self.add_module(f"enc_{idx}", enc)
        self.enc_commands = Encoder(emb_size, enc_output_size)

    def encode_sequences(self, batches):
        """
        Forward pass of Preprocessor
        :param batches: list of tuples with variable-length sequences of word ids
        :return: tensor with concatenated encoder outputs for every batch sample
        """
        data = []
        for enc, enc_batch in zip(self.encoders, zip(*batches)):
            data.append(self._apply_encoder(enc_batch, enc))
        res_t = torch.cat(data, dim=1)
        return res_t

    def _apply_encoder(self, batch, encoder):
        ord_batch, inv_batch = order_batch(batch)
        dev = self.emb.weight.device
        ord_batch_t = [torch.tensor(sample).to(dev) for sample in ord_batch]
        batch_seq = rnn_utils.pack_sequence(ord_batch_t)
        emb_seq_t = rnn_utils.PackedSequence(data=self.emb(batch_seq.data), batch_sizes=batch_seq.batch_sizes)
        res = encoder(emb_seq_t)
        res = res[inv_batch]
        return res

    def encode_commands(self, batch):
        """
        Apply encoder to list of commands sequence
        :param batch: list of lists of idx
        :return: tensor with encoded commands in original order
        """
        return self._apply_encoder(batch, self.enc_commands)
def order_batch(batch):
    """
    Order batch of sequences by sequence len and keep inverse
    :param batch: list of lists of items
    :return: Tuple[sorted batch, numpy array to return the order]
    """
    lens = list(map(len, batch))
    ord_idx = np.flip(np.argsort(lens, kind='stable'))
    ord_inv = np.argsort(ord_idx, kind='stable')
    ord_batch = [batch[idx] for idx in ord_idx]
    return ord_batch, ord_inv



class DQNModel(nn.Module):
    def __init__(self, obs_size: int, cmd_size: int, hid_size: int = 256):
        super(DQNModel, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(obs_size + cmd_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1)
        )

    def forward(self, obs, cmd):
        x = torch.cat((obs, cmd), dim=1)
        return self.net(x)

    def q_values(self, obs_t, commands_t):
        """
        Calculate q-values for observation and tensor of commands
        :param obs_t: preprocessed observation, need to be of [1, obs_size] shape
        :param commands_t: commands to be evaluated, shape is [N, cmd_size]
        :return: list of q-values for commands
        """
        result = []
        for cmd_t in commands_t:
            result.append(self(obs_t, cmd_t.unsqueeze(0))[0].cpu().item())
        return result
class DQNAgent(ptan.agent.BaseAgent):
    def __init__(self, net: DQNModel, preprocessor: Preprocessor, epsilon: float = 0.0, device="cpu"):
        self.net = net
        self.preprocessor = preprocessor
        self._epsilon = epsilon
        self.device = device

    @property
    def epsilon(self):
        return self._epsilon

    @epsilon.setter
    def epsilon(self, value: float):
        if 0.0 <= value <= 1.0:
            self._epsilon = value

    @torch.no_grad()
    def __call__(self, states, agent_states=None):
        if agent_states is None:
            agent_states = [None] * len(states)

        # for every state in the batch, calculate
        actions = []
        for state in states:
            commands = state['admissible_commands']
            if random.random() <= self.epsilon:
                actions.append(random.randrange(len(commands)))
            else:
                obs_t = self.preprocessor.encode_sequences([state['obs']]).to(self.device)
                commands_t = self.preprocessor.encode_commands(commands).to(self.device)
                q_vals = self.net.q_values(obs_t, commands_t)
                actions.append(np.argmax(q_vals))
        return actions, agent_states
def calc_loss_dqn(batch, preprocessor, tgt_preprocessor, net, tgt_net, gamma, device="cpu"):
    observations, taken_commands, rewards, next_best_qs = unpack_batch(batch, tgt_preprocessor, tgt_net, device)

    obs_t = preprocessor.encode_sequences(observations).to(device)
    cmds_t = preprocessor.encode_commands(taken_commands).to(device)
    q_values_t = net(obs_t, cmds_t)
    q_values_t = q_values_t.type(torch.cuda.FloatTensor)

    tgt_q_t = torch.tensor(rewards) + gamma * torch.tensor(next_best_qs)
    tgt_q_t = tgt_q_t.to(device)
    tgt_q_t = tgt_q_t.type(torch.cuda.FloatTensor)
    # print(q_values_t.squeeze(-1),tgt_q_t)

    return F.mse_loss(q_values_t.squeeze(-1), tgt_q_t)
@torch.no_grad()
def unpack_batch(batch: List[ptan.experience.Experience], preprocessor: Preprocessor,
                      net: DQNModel, device="cpu"):
    """
    Convert batch to data needed for Bellman step
    :param batch: list of ptan.Experience objects
    :param preprocessor: emb.Preprocessor instance
    :param net: network to be used for next state approximation
    :param device: torch device
    :return: tuple (list of observations, list of taken commands, list of rewards, list of best Qs for the next state)
    """
    # calculate Qs for next states
    observations, taken_commands, rewards, best_q = [], [], [], []
    last_obs, last_commands, last_offsets = [], [], []
    for exp in batch:
        observations.append(exp.state['obs'])
        taken_commands.append(exp.state['admissible_commands'][exp.action])
        rewards.append(exp.reward)

        # calculate best Q value for the next state
        if exp.last_state is None:
            # final state in the episode, Q=0
            last_offsets.append(len(last_commands))
        else:
            last_obs.append(exp.last_state['obs'])
            last_commands.extend(exp.last_state['admissible_commands'])
            last_offsets.append(len(last_commands))

    obs_t = preprocessor.encode_sequences(last_obs).to(device)
    commands_t = preprocessor.encode_commands(last_commands).to(device)

    prev_ofs = 0
    obs_ofs = 0
    for ofs in last_offsets:
        if prev_ofs == ofs:
            best_q.append(0.0)
        else:
            q_vals = net.q_values(obs_t[obs_ofs:obs_ofs+1], commands_t[prev_ofs:ofs])
            best_q.append(max(q_vals))
            obs_ofs += 1
        prev_ofs = ofs
    return observations, taken_commands, rewards, best_q

"""## Training loop

Ok, we're now fully prepared for training our agent, whee!
"""



for world_size in (3,5,7):
  for nb_objects in (3,5,7,9,11): 
    for quest_length in (3,5,7):
      for seed in (42,1234):
        mainfold = "/content/drive/MyDrive/mlheart/"
        gamefold = mainfold+"games/"
        gamename = "o"+str(world_size)+"r"+str(nb_objects)+"q"+str(quest_length)+"game"+str(seed)+".ulx"
        cmd = "tw-make custom --world-size "+str(world_size)+" --nb-objects "+str(nb_objects)+" --quest-length "+str(quest_length)+" --seed "+str(seed)+" --output "+gamefold +gamename
        os.system(cmd)
        print (gamename)
        traininglog=""
        file_traininglog = '/content/drive/MyDrive/mlheart/traininglogs/'+gamename+'.training.txt'
        f_t=open(file_traininglog, 'w')
        file_playlog = '/content/drive/MyDrive/mlheart/playlogs/'+gamename+'.play.txt'
        f_p=open(file_playlog, 'w')


        GAME_FILE = gamefold+gamename

        def make_env():
          game_path = pathlib.Path(GAME_FILE)
          env_id = tw_gym.register_game(
              str(game_path), max_episode_steps=STEPS_LIMIT,
              name=game_path.stem, request_infos=EnvInfos(**EXTRA_GAME_INFO))
          env = gym.make(env_id)
          return env
        env = make_env()
        device = torch.device("cuda")  # you should use GPU, CPU is way too slow
        env = make_env()
        env = TextWorldPreproc(env)

        prep = Preprocessor(dict_size=env.observation_space.vocab_size, emb_size=EMB_SIZE, 
                            num_sequences=env.num_fields, enc_output_size=ENC_SIZE)
        prep = prep.to(device)
        # we'll use target network to disentangle target predictions in Bellman
        tgt_prep = ptan.agent.TargetNet(prep)

        net = DQNModel(obs_size=env.num_fields*ENC_SIZE, cmd_size=ENC_SIZE)
        net = net.to(device)
        tgt_net = ptan.agent.TargetNet(net)

        agent = DQNAgent(net, prep, epsilon=INITIAL_EPSILON, device=device)
        exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=1)
        buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)

        optimizer = optim.RMSprop(itertools.chain(net.parameters(), prep.parameters()),
                                  lr=LEARNING_RATE, eps=1e-5)

        steps_done = 0
        episodes_done = 0
        losses = []
        rewards = []
        prev_steps = 0
        start_ts = prev_ts = time.time()

        for _ in range(10000):
            steps_done += 1
            buffer.populate(1)
            rewards_steps = exp_source.pop_rewards_steps()
            if rewards_steps:
                speed = (steps_done - prev_steps) / (time.time() - prev_ts)
                prev_steps = steps_done
                prev_ts = time.time()
                for rw, steps in rewards_steps:
                    episodes_done += 1
                    # print("%d: Done %d episodes: reward = %.2f, steps = %d, speed = %.2f steps/sec, epsilon = %.2f" % (
                    # steps_done, episodes_done, rw, steps, speed, agent.epsilon))
                    traininglog = traininglog +"%d: Done %d episodes: reward = %.2f, steps = %d, speed = %.2f steps/sec, epsilon = %.2f \n" % (                steps_done, episodes_done, rw, steps, speed, agent.epsilon)
                    rewards.append(rw)
                if rewards and np.mean(rewards[-10:]) == 6.0:
                    # print("Environment has been solved in %s, congrats!" % datetime.timedelta(seconds=time.time() - start_ts))
                    traininglog = traininglog +"Environment has been solved in %s, congrats!" % datetime.timedelta(seconds=time.time() - start_ts)
                    f_t.write(traininglog)
                    break
            if len(buffer) < REPLAY_INITIAL:
                continue
                
            batch = buffer.sample(BATCH_SIZE)
            optimizer.zero_grad()
            loss_t = calc_loss_dqn(batch, prep, tgt_prep.target_model,
                                  net, tgt_net.target_model, GAMMA, device=device)
            loss_t.backward()
            optimizer.step()
            losses.append(loss_t.item())
            
            if steps_done % SYNC_NETS == 0:
                tgt_prep.sync()
                tgt_net.sync()
                # print("%d: sync nets" % steps_done)
                
            agent.epsilon = max(FINAL_EPSILON, INITIAL_EPSILON - steps_done / STEPS_EPSILON)

        test_env = make_env()
        test_env = TextWorldPreproc(test_env)
        test_agent = DQNAgent(net, prep, epsilon=0, device=device)
        total_reward = 0
        step_idx = 0
        s = test_env.reset()  
        while True:
            step_idx += 1
            actions, _ = test_agent([s])
            action = actions[0]
            action_text = test_env.last_admissible_commands[action]
            s, r, is_done, _ = test_env.step(action)
            # print(test_env.last_extra_info)
            f_p.write(str(test_env.last_extra_info['description']))
            total_reward += r
            # print("%d: %s -> reward=%s, total_reward=%s" % (step_idx,  action_text, r, total_reward))
            f_p.write("%d: %s -> reward=%s, total_reward=%s" % (step_idx,  action_text, r, total_reward))
            if is_done:
                break
        f_t.close()
        f_p.close()

with open(file_playlog,'r') as testwritefile:
    print(testwritefile.read())

